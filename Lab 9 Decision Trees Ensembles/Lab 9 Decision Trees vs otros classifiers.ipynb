{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9 Decision Trees vs otros classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #panda dataframes\n",
    "\n",
    "Location = r'Tabla_impago_full.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importo tabla "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tabla1 = pd.read_excel(Location,sheet_name=\"Tabla_B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leo tabla ignoro los IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Tabla1.iloc[:,2:]\n",
    "data=data.fillna(data.mean())\n",
    "\n",
    "data1=data.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separo la variable dependiente de las explicativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs=data.iloc[:,1:]\n",
    "Y=data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separo mi base de datos en dos muestras, una muestra de entrenamiento y una de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, Y, test_size=0.5, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "tree.export_graphviz(clf,out_file='tree.dot')\n",
    "\n",
    "## se puede visualizar copiando y pegando el contenido del archivo .dot en http://www.webgraphviz.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importancia de las variables en el árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance=clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones y probabilidades en la muestra de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train=clf.predict(X_train)              #predicción\n",
    "probs_train=clf.predict_proba(X_train)          #probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Qué regla o nodo se activa para cada predicción?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules=clf.apply(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando esta función puedo escribir el árbol como reglas de decisión (lenguaje humano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import _tree\n",
    "def tree_to_code(tree, feature_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    print \"def tree({}):\".format(\", \".join(feature_names))\n",
    "\n",
    "    def recurse(node, depth):\n",
    "        indent = \"  \" * depth\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            print \"{}if {} <= {}:\".format(indent, name, threshold)\n",
    "            recurse(tree_.children_left[node], depth + 1)\n",
    "            print \"{}else:  # if {} > {}\".format(indent, name, threshold)\n",
    "            recurse(tree_.children_right[node], depth + 1)\n",
    "        else:\n",
    "            print \"{}return {}\".format(indent, tree_.value[node])\n",
    "\n",
    "    recurse(0, 1)\n",
    "\n",
    "# la función debe ejecutarse desde el terminal: tree_to_code(clf,X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculemos el ajuste accuracy del modelo en train y en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs2=clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Una vez que ya tengo las predicciones puedo calcular matriz de confusión, accuracy, recall, precision, auc, etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = clf.score(X_train, y_train) \n",
    "score_testing = clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para calcular otras métricas precision, recall, etc necesito también predecir en testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_testing=clf.predict(X_test)               #predigo en base testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculo recall, precision, auc, fpr, tpr, auc, f1 etc en ambas bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_train, tpr_train, thresholds_train = metrics.roc_curve(y_train, pred_train, pos_label=1)\n",
    "fpr_test, tpr_test, thresholds_test = metrics.roc_curve(y_test, pred_testing, pos_label=1)\n",
    "\n",
    "auc_train=metrics.auc(fpr_train, tpr_train)\n",
    "auc_test=metrics.auc(fpr_test, tpr_test)\n",
    "\n",
    "recalls_train=recall_score(y_train, pred_train, average=None)\n",
    "recalls_test=recall_score(y_test, pred_testing, average=None)\n",
    "\n",
    "precis_train=precision_score(y_train, pred_train, average=None)\n",
    "precis_test=precision_score(y_test, pred_testing, average=None)\n",
    "\n",
    "f1sc_train=f1_score(y_train, pred_train, average=None)\n",
    "f1sc_test=f1_score(y_test, pred_testing, average=None) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
